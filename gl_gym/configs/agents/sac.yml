TomatoEnv:
  total_timesteps: 2_000_000
  n_envs: 8
  policy: MlpPolicy
  learning_rate: 7.e-4
  buffer_size: 576_100  
  learning_starts: 57_610 # let the agents randomly explore for 10 episodes.
  batch_size: 128
  tau: 0.0135
  gamma: 0.9631
  ent_coef: auto
  train_freq: 50
  gradient_steps: 10
  action_noise: 
    normalactionnoise:
      sigma: 0.05
  replay_buffer_class: null
  replay_buffer_kwargs: null
  optimize_memory_usage: False
  target_update_interval: 1
  use_sde: False
  sde_sample_freq: -1
  use_sde_at_warmup: False
  stats_window_size: 100

  policy_kwargs: {net_arch: {pi: [256, 256, 256], qf: [512, 512, 512]},
                  optimizer_class: adam,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: silu,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }
LettuceEnv:
  total_timesteps: 2_000_000
  n_envs: 8
  policy: MlpPolicy
  learning_rate: 7.e-4
  buffer_size: 576_100  
  learning_starts: 57_610 # let the agents randomly explore for 10 episodes.
  batch_size: 128
  tau: 0.0135
  gamma: 0.9631
  ent_coef: auto
  train_freq: 50
  gradient_steps: 10
  action_noise: 
    normalactionnoise:
      sigma: 0.05
  replay_buffer_class: null
  replay_buffer_kwargs: null
  optimize_memory_usage: False
  target_update_interval: 1
  use_sde: False
  sde_sample_freq: -1
  use_sde_at_warmup: False
  stats_window_size: 100

  policy_kwargs: {net_arch: {pi: [256, 256, 256], qf: [512, 512, 512]},
                  optimizer_class: adam,
                  optimizer_kwargs: {amsgrad: True},
                  activation_fn: silu,
                  log_std_init: np.log(1) # Results in policy standard deviation of 0.5 since exp(log(0.5)) = 0.5, where np.log(1) results in std of 1
          }