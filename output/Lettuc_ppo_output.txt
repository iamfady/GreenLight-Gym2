Tuning: False
Using cpu device
Logging to train_data/AgriControl/ppo/deterministic/logs/devout-river-76/PPO_0
------------------------------
| time/              |       |
|    fps             | 1748  |
|    iterations      | 1     |
|    time_elapsed    | 9     |
|    total_timesteps | 16384 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 1274        |
|    iterations           | 2           |
|    time_elapsed         | 25          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.016313959 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.0237      |
|    learning_rate        | 0.0001      |
|    loss                 | -0.104      |
|    n_updates            | 8           |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.01        |
|    value_loss           | 0.176       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 1469.8237   |
| time/                   |             |
|    fps                  | 1198        |
|    iterations           | 3           |
|    time_elapsed         | 41          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.017870937 |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.56       |
|    explained_variance   | 0.491       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.101      |
|    n_updates            | 16          |
|    policy_gradient_loss | -0.0228     |
|    std                  | 1.01        |
|    value_loss           | 0.0255      |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=2253.40 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -14.8       |
|    co2_cost             | 1.19        |
|    co2_violation        | 0           |
|    elec_cost            | 1.28        |
|    fixed_costs          | 4.13        |
|    heat_cost            | 14          |
|    mean_reward          | 2.25e+03    |
|    revenue              | 1.67        |
|    rh_violation         | 9.74e+03    |
|    temp_violation       | 1.3e+03     |
|    variable_costs       | 16.4        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.017763153 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.58       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.113      |
|    n_updates            | 24          |
|    policy_gradient_loss | -0.0195     |
|    std                  | 1.01        |
|    value_loss           | 0.0312      |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 1469.8237 |
| time/              |           |
|    fps             | 1024      |
|    iterations      | 4         |
|    time_elapsed    | 63        |
|    total_timesteps | 65536     |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 1469.8237   |
| time/                   |             |
|    fps                  | 1071        |
|    iterations           | 5           |
|    time_elapsed         | 76          |
|    total_timesteps      | 81920       |
| train/                  |             |
|    approx_kl            | 0.021359997 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.114      |
|    n_updates            | 32          |
|    policy_gradient_loss | -0.0343     |
|    std                  | 1.01        |
|    value_loss           | 0.029       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 1888.8296   |
| time/                   |             |
|    fps                  | 1067        |
|    iterations           | 6           |
|    time_elapsed         | 92          |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.019983921 |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.102      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0257     |
|    std                  | 1.01        |
|    value_loss           | 0.0147      |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=2832.71 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -6.69       |
|    co2_cost             | 0.00591     |
|    co2_violation        | 0           |
|    elec_cost            | 0.153       |
|    fixed_costs          | 4.13        |
|    heat_cost            | 7.8         |
|    mean_reward          | 2.83e+03    |
|    revenue              | 1.27        |
|    rh_violation         | 7.68e+03    |
|    temp_violation       | 718         |
|    variable_costs       | 7.96        |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.018279415 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0973     |
|    n_updates            | 48          |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1.01        |
|    value_loss           | 0.0238      |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 1888.8296 |
| time/              |           |
|    fps             | 990       |
|    iterations      | 7         |
|    time_elapsed    | 115       |
|    total_timesteps | 114688    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 1888.8296   |
| time/                   |             |
|    fps                  | 1011        |
|    iterations           | 8           |
|    time_elapsed         | 129         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.017750848 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.6        |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.102      |
|    n_updates            | 56          |
|    policy_gradient_loss | -0.0222     |
|    std                  | 1.01        |
|    value_loss           | 0.011       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.76e+03   |
|    ep_rew_mean          | 2189.0703  |
| time/                   |            |
|    fps                  | 1011       |
|    iterations           | 9          |
|    time_elapsed         | 145        |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.01983929 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.59      |
|    explained_variance   | 0.853      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.102     |
|    n_updates            | 64         |
|    policy_gradient_loss | -0.0238    |
|    std                  | 1.01       |
|    value_loss           | 0.0117     |
----------------------------------------
Eval num_timesteps=150000, episode_reward=3373.06 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -3.7        |
|    co2_cost             | 0.00132     |
|    co2_violation        | 0           |
|    elec_cost            | 0.00892     |
|    fixed_costs          | 4.13        |
|    heat_cost            | 4.86        |
|    mean_reward          | 3.37e+03    |
|    revenue              | 1.17        |
|    rh_violation         | 1.97e+03    |
|    temp_violation       | 560         |
|    variable_costs       | 4.87        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.020235239 |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.59       |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0847     |
|    n_updates            | 72          |
|    policy_gradient_loss | -0.0184     |
|    std                  | 1.01        |
|    value_loss           | 0.0155      |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 2189.0703 |
| time/              |           |
|    fps             | 970       |
|    iterations      | 10        |
|    time_elapsed    | 168       |
|    total_timesteps | 163840    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2189.0703   |
| time/                   |             |
|    fps                  | 990         |
|    iterations           | 11          |
|    time_elapsed         | 181         |
|    total_timesteps      | 180224      |
| train/                  |             |
|    approx_kl            | 0.019923551 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.58       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.103      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0212     |
|    std                  | 1.01        |
|    value_loss           | 0.00605     |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.76e+03   |
|    ep_rew_mean          | 2425.9817  |
| time/                   |            |
|    fps                  | 1001       |
|    iterations           | 12         |
|    time_elapsed         | 196        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.01863073 |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.57      |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.102     |
|    n_updates            | 88         |
|    policy_gradient_loss | -0.0206    |
|    std                  | 1.01       |
|    value_loss           | 0.00763    |
----------------------------------------
Eval num_timesteps=200000, episode_reward=3495.30 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -3.06       |
|    co2_cost             | 0.000278    |
|    co2_violation        | 0           |
|    elec_cost            | 0.0107      |
|    fixed_costs          | 4.13        |
|    heat_cost            | 4.22        |
|    mean_reward          | 3.5e+03     |
|    revenue              | 1.17        |
|    rh_violation         | 907         |
|    temp_violation       | 271         |
|    variable_costs       | 4.23        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.021062717 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.57       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.129      |
|    n_updates            | 96          |
|    policy_gradient_loss | -0.0159     |
|    std                  | 1.01        |
|    value_loss           | 0.00574     |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 2425.9817 |
| time/              |           |
|    fps             | 971       |
|    iterations      | 13        |
|    time_elapsed    | 219       |
|    total_timesteps | 212992    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2425.9817   |
| time/                   |             |
|    fps                  | 984         |
|    iterations           | 14          |
|    time_elapsed         | 232         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.018943049 |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.55       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.12       |
|    n_updates            | 104         |
|    policy_gradient_loss | -0.0231     |
|    std                  | 1.01        |
|    value_loss           | 0.00364     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2600.5784   |
| time/                   |             |
|    fps                  | 994         |
|    iterations           | 15          |
|    time_elapsed         | 247         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.020430347 |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.54       |
|    explained_variance   | 0.873       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.112      |
|    n_updates            | 112         |
|    policy_gradient_loss | -0.0188     |
|    std                  | 1           |
|    value_loss           | 0.00314     |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=3518.36 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -2.02       |
|    co2_cost             | 0.00336     |
|    co2_violation        | 0           |
|    elec_cost            | 0.00587     |
|    fixed_costs          | 4.13        |
|    heat_cost            | 3.11        |
|    mean_reward          | 3.52e+03    |
|    revenue              | 1.1         |
|    rh_violation         | 1.39e+03    |
|    temp_violation       | 223         |
|    variable_costs       | 3.12        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.022512067 |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.53       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.112      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0121     |
|    std                  | 1           |
|    value_loss           | 0.00193     |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 2600.5784 |
| time/              |           |
|    fps             | 971       |
|    iterations      | 16        |
|    time_elapsed    | 269       |
|    total_timesteps | 262144    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2728.0867   |
| time/                   |             |
|    fps                  | 982         |
|    iterations           | 17          |
|    time_elapsed         | 283         |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.021089397 |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.853       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.102      |
|    n_updates            | 128         |
|    policy_gradient_loss | -0.0196     |
|    std                  | 1           |
|    value_loss           | 0.00288     |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2728.0867   |
| time/                   |             |
|    fps                  | 988         |
|    iterations           | 18          |
|    time_elapsed         | 298         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.022280786 |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.852       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0909     |
|    n_updates            | 136         |
|    policy_gradient_loss | -0.0136     |
|    std                  | 1           |
|    value_loss           | 0.00326     |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=3555.92 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -2.32       |
|    co2_cost             | 0.00467     |
|    co2_violation        | 0           |
|    elec_cost            | 0           |
|    fixed_costs          | 4.13        |
|    heat_cost            | 3.39        |
|    mean_reward          | 3.56e+03    |
|    revenue              | 1.07        |
|    rh_violation         | 539         |
|    temp_violation       | 284         |
|    variable_costs       | 3.39        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.020720776 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.52       |
|    explained_variance   | 0.949       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0964     |
|    n_updates            | 144         |
|    policy_gradient_loss | -0.0169     |
|    std                  | 1           |
|    value_loss           | 0.00118     |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 2728.0867 |
| time/              |           |
|    fps             | 971       |
|    iterations      | 19        |
|    time_elapsed    | 320       |
|    total_timesteps | 311296    |
----------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 5.76e+03   |
|    ep_rew_mean          | 2830.185   |
| time/                   |            |
|    fps                  | 979        |
|    iterations           | 20         |
|    time_elapsed         | 334        |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.02185031 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.49      |
|    explained_variance   | 0.887      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.108     |
|    n_updates            | 152        |
|    policy_gradient_loss | -0.023     |
|    std                  | 0.996      |
|    value_loss           | 0.00161    |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2830.185    |
| time/                   |             |
|    fps                  | 987         |
|    iterations           | 21          |
|    time_elapsed         | 348         |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.023654345 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.48       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.106      |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.0115     |
|    std                  | 0.994       |
|    value_loss           | 0.00167     |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=3574.57 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -2.44       |
|    co2_cost             | 0.00476     |
|    co2_violation        | 0           |
|    elec_cost            | 0.00016     |
|    fixed_costs          | 4.13        |
|    heat_cost            | 3.44        |
|    mean_reward          | 3.57e+03    |
|    revenue              | 1           |
|    rh_violation         | 330         |
|    temp_violation       | 121         |
|    variable_costs       | 3.44        |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.020210274 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.46       |
|    explained_variance   | 0.954       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0931     |
|    n_updates            | 168         |
|    policy_gradient_loss | -0.0164     |
|    std                  | 0.99        |
|    value_loss           | 0.000726    |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.76e+03 |
|    ep_rew_mean     | 2830.185 |
| time/              |          |
|    fps             | 971      |
|    iterations      | 22       |
|    time_elapsed    | 371      |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2911.6963   |
| time/                   |             |
|    fps                  | 979         |
|    iterations           | 23          |
|    time_elapsed         | 384         |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.022354513 |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.44       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.097      |
|    n_updates            | 176         |
|    policy_gradient_loss | -0.0183     |
|    std                  | 0.988       |
|    value_loss           | 0.000937    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2911.6963   |
| time/                   |             |
|    fps                  | 981         |
|    iterations           | 24          |
|    time_elapsed         | 400         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.023510145 |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.43       |
|    explained_variance   | 0.907       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0941     |
|    n_updates            | 184         |
|    policy_gradient_loss | -0.0095     |
|    std                  | 0.988       |
|    value_loss           | 0.00139     |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=3564.46 +/- 0.00
----------------------------------------
| eval/                   |            |
|    EPI                  | -2.11      |
|    co2_cost             | 0.000691   |
|    co2_violation        | 0          |
|    elec_cost            | 0          |
|    fixed_costs          | 4.13       |
|    heat_cost            | 3.13       |
|    mean_reward          | 3.56e+03   |
|    revenue              | 1.02       |
|    rh_violation         | 703        |
|    temp_violation       | 144        |
|    variable_costs       | 3.13       |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.02440647 |
|    clip_fraction        | 0.266      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.43      |
|    explained_variance   | 0.958      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0918    |
|    n_updates            | 192        |
|    policy_gradient_loss | -0.0183    |
|    std                  | 0.987      |
|    value_loss           | 0.000652   |
----------------------------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 2911.6963 |
| time/              |           |
|    fps             | 963       |
|    iterations      | 25        |
|    time_elapsed    | 425       |
|    total_timesteps | 409600    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2977.4082   |
| time/                   |             |
|    fps                  | 970         |
|    iterations           | 26          |
|    time_elapsed         | 438         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.018035332 |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.41       |
|    explained_variance   | 0.9         |
|    learning_rate        | 0.0001      |
|    loss                 | -0.113      |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.0156     |
|    std                  | 0.985       |
|    value_loss           | 0.000715    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 2977.4082   |
| time/                   |             |
|    fps                  | 976         |
|    iterations           | 27          |
|    time_elapsed         | 453         |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.025456619 |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.42       |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0781     |
|    n_updates            | 208         |
|    policy_gradient_loss | -0.00807    |
|    std                  | 0.989       |
|    value_loss           | 0.00126     |
-----------------------------------------
Eval num_timesteps=450000, episode_reward=3577.51 +/- 0.00
----------------------------------------
| eval/                   |            |
|    EPI                  | -1.9       |
|    co2_cost             | 0          |
|    co2_violation        | 0          |
|    elec_cost            | 0.000851   |
|    fixed_costs          | 4.13       |
|    heat_cost            | 2.87       |
|    mean_reward          | 3.58e+03   |
|    revenue              | 0.977      |
|    rh_violation         | 651        |
|    temp_violation       | 161        |
|    variable_costs       | 2.87       |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.02352295 |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    entropy_loss         | -8.42      |
|    explained_variance   | 0.955      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.11      |
|    n_updates            | 216        |
|    policy_gradient_loss | -0.0182    |
|    std                  | 0.987      |
|    value_loss           | 0.000748   |
----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 2977.4082 |
| time/              |           |
|    fps             | 961       |
|    iterations      | 28        |
|    time_elapsed    | 477       |
|    total_timesteps | 458752    |
----------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 3031.5457   |
| time/                   |             |
|    fps                  | 966         |
|    iterations           | 29          |
|    time_elapsed         | 491         |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.020842843 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.41       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.11       |
|    n_updates            | 224         |
|    policy_gradient_loss | -0.019      |
|    std                  | 0.984       |
|    value_loss           | 0.000693    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.76e+03    |
|    ep_rew_mean          | 3031.5457   |
| time/                   |             |
|    fps                  | 968         |
|    iterations           | 30          |
|    time_elapsed         | 507         |
|    total_timesteps      | 491520      |
| train/                  |             |
|    approx_kl            | 0.028839039 |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.41       |
|    explained_variance   | 0.918       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.111      |
|    n_updates            | 232         |
|    policy_gradient_loss | -0.00436    |
|    std                  | 0.987       |
|    value_loss           | 0.000832    |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=3586.24 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    EPI                  | -1.92       |
|    co2_cost             | 0           |
|    co2_violation        | 0           |
|    elec_cost            | 0.00256     |
|    fixed_costs          | 4.13        |
|    heat_cost            | 2.89        |
|    mean_reward          | 3.59e+03    |
|    revenue              | 0.97        |
|    rh_violation         | 554         |
|    temp_violation       | 112         |
|    variable_costs       | 2.89        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.021523315 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -8.4        |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.121      |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.0174     |
|    std                  | 0.984       |
|    value_loss           | 0.000716    |
-----------------------------------------
New best mean reward!
-----------------------
Saving VecNormalize to train_data/AgriControl/ppo/deterministic/envs/devout-river-76/best_vecnormalize.pkl
-----------------------
----------------------------------
| rollout/           |           |
|    ep_len_mean     | 5.76e+03  |
|    ep_rew_mean     | 3077.0818 |
| time/              |           |
|    fps             | 954       |
|    iterations      | 31        |
|    time_elapsed    | 531       |
|    total_timesteps | 507904    |
----------------------------------
